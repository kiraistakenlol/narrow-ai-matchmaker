# Comparison of Text Embedding API Providers (OpenAI, Cohere, Google Vertex AI, AWS Bedrock)

| **Criteria**                 | **OpenAI API (e.g. Ada / text-embedding-3 models)**                                                                                                                                                                                                                                                                                       | **Cohere API (Embed v3 & v4 models)**                                                                                                                                                                                                                                                                                                                                                     | **Google Vertex AI (Generative AI Embeddings)**                                                                                                                                                                                                                                                                                                                                                        | **AWS Bedrock (Amazon Titan Embeddings)**                                                                                                                                                                                                                                                                                                                                                        |
| :--------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Models Offered & Performance** | **Models:** Offers `text-embedding-ada-002` (1536-dim) and newer `text-embedding-3` models in "small" and "large" variants. Small model is efficient Ada-002 successor; large model produces 3072-dim embeddings. <br> **Quality:** State-of-the-art semantic embeddings. New `3-large` leads benchmarks (MTEB ~64.6 vs 61.0 for Ada-002), greatly improves multilingual retrieval. Even `3-small` (1536-dim) slightly outperforms Ada-002 (MTEB ~62.3). | **Models:** Embed v3 family for English (`embed-english-v3.0`, 1024-dim; `embed-english-light-v3.0`, 384-dim) and multilingual (`embed-multilingual-v3.0`, 1024-dim; light, 384-dim). Newer Embed v4 (multimodal) supports text & image, up to 1536-dim. <br> **Quality:** Embeddings on par with OpenAI. `embed-english-v3.0` scores ~64.5 on MTEB. Models excel in English nuances, support 100+ languages. Embed v4 reported state-of-the-art in search/retrieval. | **Models:** Gecko series (`textembedding-gecko@001…003`, ~768-dim, fast). Newer `text-embedding-005` (general) and `text-multilingual-embedding-002` (multi-language). Experimental Gemini Embedding (`text-embedding-large-exp-03-07`, ≈3000-dim). <br> **Quality:** Gemini achieves state-of-the-art semantic accuracy (MTEB ~68.3 Multilingual), outperforming others. Handles 100+ languages, complex context. Smaller Gecko models perform well. | **Models:** Titan Text Embeddings V2 (GA Apr 2024). Lightweight, optimized for RAG/search. Flexible output up to 1,024 dimensions (options: 256/512). Pre-trained on 100+ languages. <br> **Quality:** High semantic accuracy approaching top models (v2 closed gap vs v1). Well-suited for tech domain texts. Independent benchmarks for v2 still emerging.                                                                   |
| **Pricing**                  | **Cost model:** Pay-per-use by input tokens. Ada-002: $0.0001/1K tokens. New models cheaper: `text-embedding-3-small` $0.00002/1K tokens (5x lower); `3-large` $0.00013/1K tokens. <br> **Free tier:** No permanent free tier (discontinued free trial credits). Pay-as-you-go from start.                                                    | **Cost model:** Billed per input token. Rate ~ $0.0001/1K tokens (~$0.10/million). English & multilingual models share similar pricing. <br> **Free tier:** Developer-friendly free tier for prototyping (rate-limited). Paid plans for production. Allows trying at no cost initially.                                                                                                                            | **Cost model:** Charges based on input characters. ~ $0.000025/1K chars (online inference); ~ $0.00002/1K chars (batch, ~20% cheaper). ~ Equivalent to $0.0001/1K tokens. No charge for output vectors. <br> **Free tier:** $300 free credits for new Google Cloud users. After credits, pay-as-you-go.                                                                                                   | **Cost model:** Pay-as-you-go per input token. ~ $0.0001/1K tokens (~$0.10/1M). No additional fees for Bedrock service itself. <br> **Free tier:** No official free tier for Bedrock (not in AWS free tier). Charges from first request, though AWS credits may apply.                                                                                                                               |
| **Latency**                  | **Typical latency:** Very low (< 300ms for short inputs). Production-hardened. Supports batch embedding (up to 2048 texts/request) to amortize overhead. <br> **Notes:** Acceptable for offline embedding. Easily handles batch jobs.                                                                                                       | **Typical latency:** Millisecond-level (~100–200ms or faster). Optimized for real-time search. Also offers asynchronous batch "Embed Jobs" API for high throughput. <br> **Notes:** Enterprise-grade scalability. Reliably handles bulk embedding. Latency rarely a bottleneck for offline.                                                                                                                  | **Typical latency:** Gecko models ~100ms (low latency). Gemini (large) model higher latency (few hundred ms), acceptable for non-real-time. <br> **Notes:** Vertex AI supports batch processing (more efficient, lower cost). Minimal network latency if hosted on GCP.                                                                                                                                            | **Typical latency:** Comparable to others (~few hundred ms or less for short paragraph). Titan v2 described as "lightweight", efficient. Bedrock added batch mode support (~50% cheaper) for throughput. <br> **Notes:** Very low network latency if backend is on AWS (same region, esp. via PrivateLink). Service can scale for large document sets.                                               |
| **Ease of Integration**      | **SDKs & REST:** Very easy. Official `openai` Python SDK. Call `openai.Embedding.create(...)`. Abundant examples, tutorials, community libraries (LangChain). <br> **Documentation:** Clear API docs, code snippets. Widely shared troubleshooting. Minimal implementation effort.                                                             | **SDKs & REST:** Intuitive `cohere` Python SDK (`co.embed(...)`). RESTful API. Web Playground for testing. <br> **Documentation:** Developer-friendly, quickstart guides. Growing ecosystem (LangChain support). Easy to work with, though less ubiquitous than OpenAI.                                                                                                                                     | **SDKs & REST:** Requires GCP project. Use Vertex AI Python SDK (`google.cloud.aiplatform`) or REST/gRPC. Needs setup (enable API, credentials), but Google tooling helps. <br> **Documentation:** Extensive, thorough guides (Colab notebooks). Native integration with Vertex AI ecosystem (vector DB, etc.).                                                                                                     | **SDKs & REST:** Requires AWS account, IAM setup. Use AWS SDKs (e.g., Boto3 for Python). Call `bedrock.invoke_model`. AWS examples & CloudFormation templates available. <br> **Documentation:** AWS docs, example notebooks. Growing community examples. Smooth if already in AWS ecosystem. Main overhead is initial config (enabling Bedrock access).                                                         |
| **Vector Dimensionality**    | **Dimensions:** 1536 (Ada-002, `3-small`), 3072 (`3-large`). <br> **Adjustable?** Yes, latest models allow truncating via `dimensions` parameter (e.g., request 1024 from 3072). Even 256-dim `3-large` can outperform full Ada. Flexibility for accuracy/storage trade-off.                                                                         | **Dimensions:** Embed v3 fixed sizes (1024/384). <br> **Adjustable?** Embed v4 introduces Matryoshka embeddings: choose 256, 512, 1024, 1536 (default). Supports flexible dimensionality like OpenAI. Minimal accuracy loss reported at lower dims (~97% at 256 vs 1024).                                                                                                                                   | **Dimensions:** ~768 (Gecko models), ~3000 (Gemini large). <br> **Adjustable?** Yes, specify lower `output_dimensionality`. Gemini uses "Matryoshka Representation Learning" allowing dropping dimensions (e.g., first 1024 of 3000) while largely preserving accuracy. Useful for DB limits/space saving.                                                                                                        | **Dimensions:** Titan v1 fixed 1536. Titan v2 default 1024, flexible options: 512 or 256. Minimal accuracy loss reported (~99% retrieval at 512 vs 1024). <br> **Adjustable?** Yes, via Bedrock API `outputDimensions` parameter. Allows optimizing storage/performance in Qdrant.                                                                                                                    |
| **Data Privacy**             | **Policy:** API data NOT used for training by default. 30-day transient logs for abuse monitoring, then deleted. Secure transmission. <br> **Storage:** Cloud-based (US regions). Data leaves environment but stored transiently. OpenAI focuses on security. Default policy likely sufficient.                                                  | **Policy:** Default opt-in for training (opt-out available). 30-day logs. Enterprise zero-retention or VPC/self-host options available. <br> **Storage:** SOC2 compliant, encryption in transit. Public API with opt-out likely sufficient.                                                                                                                                                         | **Policy:** Customer data NOT used for training without permission. Short-term cache (up to 24h, disable-able). Strong enterprise compliance (GDPR). <br> **Storage:** Data stays within Google Cloud (choose regions). Profile data stays within GCP project boundaries.                                                                                                                                        | **Policy:** Content NOT used to improve base models or shared with providers. Data stays private by default. <br> **Storage:** Data remains within AWS environment/VPC. Encrypted in transit & at rest (KMS option). Bedrock attractive for privacy-conscious apps.                                                                                                                                       |
| **Key Strengths & Weaknesses** | **Pros:** Best-in-class semantic embeddings (`3-large` top accuracy, `3-small` strong & cheap). Extremely easy implementation. Huge community support. <br> **Cons:** External API dependency. Higher cost for `3-large`. Few downsides for this use-case.                                                                                        | **Pros:** Highly accurate embeddings. Multimodal (v4). Strong multi-language. Customizable deployment (self-host option). Free tier for testing. <br> **Cons:** Less ubiquitous than OpenAI (fewer integrations). Default data training opt-in (remember to opt out). Smaller community resources.                                                                                                       | **Pros:** Highest semantic accuracy (Gemini). Excellent long text/multi-language support. Deep integration with Google ML ecosystem. <br> **Cons:** Higher setup complexity. Gemini experimental (quotas?). Higher latency for Gemini. Pricing by character confusing.                                                                                                                                             | **Pros:** Ideal if on AWS (security, integration). Data stays in AWS env. Titan v2 competitive quality, flexible dims. Access other Bedrock models. <br> **Cons:** Titan v2 newer, less battle-tested. Fewer community examples. Ties you into AWS. Less "plug-and-play" community support.                                                                                                           |

## Details

These models were evaluated based on:
- **State-of-the-art semantic accuracy** (benchmarks like MTEB, MIRACL)
- **Pay-as-you-go pricing** structures
- **API latency** and batch processing support for offline embedding
- **SDK and documentation quality**
- **Embedding dimension options and truncation support**
- **Data usage policies** and privacy defaults
- **Overall pros and cons** for an offline, high-accuracy user-profile matchmaking application
